<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="Kemal Erdem">

    <title>PyTorch Basic Tutorial Part 2</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <!--    <link rel="stylesheet" href="css/highlight/isbl-editor-light.css">-->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background-transition="zoom">
            <h2>PyTorch</h2>
            <p>This is a basic tutorial about Pytorch application to computer vision</p>
            <small>Binh Nguyen</small>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title">Initial assumptions</h3>
            <p class="fragment">You know how "standard" DL training loop looks like</p>
            <p class="fragment">You (more-less) know how PyTorch works</p>

        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title">Core libraries</h3>
            <ul>
                <li class="fragment">torch (Core library)</li>
                <li class="fragment">torch.optim (Core optimizers library)</li>
                <li class="fragment">torch.nn (Core neural networks library)</li>
                <li class="fragment">torch.nn.functional (Core neural networks functions library)</li>
                <li class="fragment">torch.utils (Utilities library)</li>
                <li class="fragment">torchvision (Core vision library)</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Torch model</p>
            <pre data-id="code-animation1"><code class="hljs" data-trim data-line-numbers="|4-8|9-14">
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 10)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        return self.fc2(x)
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Loss and activation functions</p>
            <pre data-id="code-animation2"><code class="hljs" data-trim data-line-numbers="|1|3|5-8|10|11">
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

optimizer = optim.Adam([var1, var2], lr=0.0001)

optimizer = optim.SGD([
    {'params': model.base.parameters()},
    {'params': model.classifier.parameters(), 'lr': 1e-3}
    ], lr=1e-2, momentum=0.9)

criterion = nn.CrossEntropyLoss()
criterion = nn.BCELoss()
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Torch training loop</p>
            <pre data-id="code-animation3"><code class="hljs" data-trim data-line-numbers="|5|8|9|10|11">
for epoch in range(EPOCHS):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        ...
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Convolution layers</p>
            <ul class="fragment">
                <li>nn.Conv1d</li>
                <li>nn.Conv2d</li>
                <li>nn.Conv3d</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Pooling layers</p>
            <ul class="fragment">
                <li>nn.MaxPool1d</li>
                <li>nn.MaxPool2d</li>
                <li>nn.MaxPool3d</li>
            </ul>
            <ul class="fragment">
                <li>nn.AvgPool1d</li>
                <li>nn.AvgPool2d</li>
                <li>nn.AvgPool3d</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Dropout layers</p>
            <ul class="fragment">
                <li>nn.Dropout</li>
                <li>nn.Dropout2d</li>
                <li>nn.Dropout3d</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Normalization layers</p>
            <ul class="fragment">
                <li>nn.BatchNorm1d</li>
                <li>nn.BatchNorm2d</li>
                <li>nn.BatchNorm3d</li>
                <li>nn.GroupNorm</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Non-linear Activations</p>
            <ul class="fragment">
                <li>nn.ReLU</li>
                <li>nn.Sigmoid</li>
                <li>nn.ELU</li>
            </ul>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Convolution layer</p>
            <pre data-id="code-animation4" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
torch.nn.Conv2d(
    in_channels, 
    out_channels, 
    kernel_size, 
    stride=1, 
    padding=0, 
    dilation=1, 
    groups=1, 
    bias=True, 
    padding_mode='zeros')
                </code>
            </pre>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Dimension calculation</p>
            <div class="fragment">
                <script type="math/tex">
                - Input: \left(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}}\right)\\
                - Output: \left(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}}\right) where\\
                H_{out}=\left\lfloor\frac{H_{in}+2 \times \operatorname{padding}[0]-\operatorname{dilation}[0] \times(\operatorname{kernel\_size}[0]-1)-1}{\operatorname{stride}[0]}+1\right]\\
                W_{out}=\left\lfloor\frac{W_{in}+2 \times \operatorname{padding}[1]-\operatorname{dilation}[1] \times(\operatorname{kernel\_size}[1]-1)-1}{\operatorname{stride}[1]}+1\right]\\
                </script>
            </div>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Examples</p>
            <pre data-id="code-animation5" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
# With square kernels and equal stride
m = nn.Conv2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
input = torch.randn(20, 16, 50, 100)
output = m(input)
                </code>
            </pre>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title4">Exercise 1</h3>
            <p class="fragment">Create a 4-dimensional tensor 'a' with (N, C, H, W) = (4,4,4,4), a convolution layer 'conv' with kernel size of 3, 
                stride sets to 1 and padding set to 0, output channel is 32, then print out the value of output tensor after 'a' goes through 'conv'.
            </p>
            <pre data-id="code-animation6" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
a = torch.randint(256).reshape(4,4,4,4)
conv = nn.Conv2d(4, 32, kernel_size=3, stride=1, padding=0)
b = conv(a)
print(b)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Pooling layers</p>
            <pre data-id="code-animation7" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, 
    return_indices=False, ceil_mode=False)
torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, 
    count_include_pad=True, divisor_override=None)
torch.nn.AdaptiveMaxPool2d(output_size,return_indices=False)
torch.nn.AdaptiveAvgPool2d(output_size)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Dimension calculation (non adaptive)</p>
            <div class="fragment">
                <script type="math/tex">
                - Input: \left(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}}\right)\\
                - Output: \left(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}}\right) where\\
                H_{out}=\left\lfloor\frac{H_{in}+2 \times \operatorname{padding}[0]-\operatorname{kernel\_size}[0]}{\operatorname{stride}[0]}+1\right]\\
                W_{out}=\left\lfloor\frac{W_{in}+2 \times \operatorname{padding}[1]-\operatorname{kernel\_size}[1]}{\operatorname{stride}[1]}+1\right]\\
                </script>
            </div>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title5">Exercise 2</h3>
            <p class="fragment"> Using the output of exercise 1, create a max pooling layer 'pool' with kernel sets to 2, padding equals 0, 
                print out the value of output layer after 'b' goes through 'pool'.
            </p>
            <pre data-id="code-animation8" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
pool = nn.MaxPool2d(kernel_size=2, padding=0)
c = pool(b)
print(c)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title4">One more thing, I need Tensorboard logging</h3>
            <pre data-id="code-animation7" class="fragment"><code class="hljs" data-trim data-line-numbers="|1-2|5|12|16-19">
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

for epoch in range(EPOCHS):
    loss = 0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()

        outputs = net(inputs)
        # Compute loss function
        loss += criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if phase = "training":
            writer.add_scalar('Loss/train', loss)
        elif phase = "test":
            writer.add_scalar('Loss/test', loss)
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h4 data-id="code-title">Actually, I would rather use W&B then Tensorboard, can we change that?</h4>
            <pre data-id="code-animation" class="fragment"><code class="hljs" data-trim data-line-numbers="|1|5-8">
import wandb
for epoch in range(EPOCHS):
    ...

    if phase = "training":
        wandb.log({'epoch': epoch, 'train_loss': loss})
    elif phase = "test":
        wandb.log({'epoch': epoch, 'test_loss': loss})
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title">You know what?</h3>

            <p data-id="code-title">Nvidia gave us a GPU</p>

            <p data-id="code-title">Can we run this code using CUDA?</p>

            <pre data-id="code-animation" class="fragment"><code class="hljs" data-trim
                                                                 data-line-numbers="1, 2, 7, 8">

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)

for epoch in range(EPOCHS):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss += criterion(outputs, labels)  # Compute loss function
        ...
					</code></pre>
        </section>

        <section data-background-transition="zoom">
            <h3 data-id="code-title">And how to run that on multiple GPUs?</h3>
            <pre data-id="code-animation" class="fragment"><code class="hljs" data-trim
                                                                 data-line-numbers="1, 2, 7, 8">
model = model.to(device) # Not this one
model = nn.DataParallel(model, device_ids=[0,1,2,3]) # This
					</code></pre>
        </section>

        <section data-background-transition="zoom">
            <h3>Thank you for your time</h3>
            <p>Feel free to ask any question</p>
            <p>
                <small>Binh Nguyen</small><br/>
                <small>Presentation available at:<br> <a
                        href="https://beandkay.github.io/#">beandkay.github.io/#</a></small><br/>
            </p>
        </section>
    </div>


    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/math/math.js"></script>
	<script src="../plugin/math/math.js"></script>
    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        width: '75%',
        height: '100%',
        hash: true,
        controls: true,
        progress: true,
        slideNumber: true,
        overview: true,
        center: true,
        navigationMode: 'default',
        fragmentInURL: false,
        embedded: false,
        preloadIframes: null,
        autoSlide: 0,
        autoSlideStoppable: true,
        defaultTiming: 120,
        mouseWheel: false,
        previewLinks: false,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'fast', // default/fast/slow
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        display: 'block',
        math: {
          mathjax: 'plugin/math/MathJax.js',
          config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
          // pass other options into `MathJax.Hub.Config()`
          TeX: {
              Macros: {
                    RR: '{\\bf R}',
                    R: '\\mathbb{R}',
                    set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
                }
            },
        },
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],
      });
    </script>
</body>
</html>




