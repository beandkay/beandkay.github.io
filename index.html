<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="Kemal Erdem">

    <title>PyTorch Basic Tutorial Part 2</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/black.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <!--    <link rel="stylesheet" href="css/highlight/isbl-editor-light.css">-->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section data-background-transition="zoom">
            <h2>PyTorch</h2>
            <p>This is a basic tutorial about Pytorch application to computer vision</p>
            <small>Binh Nguyen</small>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title">Initial assumptions</h3>
            <p class="fragment">You know how "standard" DL training loop looks like</p>
            <p class="fragment">You (more-less) know how PyTorch works</p>

        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title">Core libraries</h3>
            <ul>
                <li class="fragment">torch (Core library)</li>
                <li class="fragment">torch.optim (Core optimizers library)</li>
                <li class="fragment">torch.nn (Core neural networks library)</li>
                <li class="fragment">torch.nn.functional (Core neural networks functions library)</li>
                <li class="fragment">torch.utils (Utilities library)</li>
                <li class="fragment">torchvision (Core vision library)</li>
                <li class="fragment">torchvision.models (Vision models library)</li>
                <li class="fragment">torchvision.transform (Vision transformation library)</li>
            </ul>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Torch model</p>
            <pre data-id="code-animation1"><code class="hljs" data-trim data-line-numbers="|4-8|9-14">
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 10)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        return self.fc2(x)
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Loss and activation functions</p>
            <pre data-id="code-animation2"><code class="hljs" data-trim data-line-numbers="|1|3|5-8|10|11">
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

optimizer = optim.Adam([var1, var2], lr=0.0001)

optimizer = optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
                ], lr=1e-2, momentum=0.9)

criterion = nn.CrossEntropyLoss()
criterion = nn.BCELoss()
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title1">Quick recap</h3>
            <p>Torch training loop</p>
            <pre data-id="code-animation3"><code class="hljs" data-trim data-line-numbers="|5|8|9|10|11">
for epoch in range(EPOCHS):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        ...
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <section>
            <h3 data-id="code-title2">Popular computer vision layers</h3>
            <p class="fragment">Convolution layers</p>
                <ul class="fragment">
                    <li>nn.Conv1d</li>
                    <li>nn.Conv2d</li>
                    <li>nn.Conv3d</li>
                </ul>
            </section>

            <section>
                <h3 data-id="code-title2">Popular computer vision layers</h3>
                <p class="fragment">Pooling layers</p>
                <ul class="fragment">
                    <li>nn.MaxPool1d</li>
                    <li>nn.MaxPool2d</li>
                    <li>nn.MaxPool3d</li>
                </ul>
                <ul class="fragment">
                    <li>nn.AvgPool1d</li>
                    <li>nn.AvgPool2d</li>
                    <li>nn.AvgPool3d</li>
                </ul>
            </section>

            <section>
                <h3 data-id="code-title2">Popular computer vision layers</h3>
                <p class="fragment">Dropout layers</p>
                <ul class="fragment">
                    <li>nn.Dropout</li>
                    <li>nn.Dropout2d</li>
                    <li>nn.Dropout3d</li>
                </ul>
            </section>

            <section>
                <h3 data-id="code-title2">Popular computer vision layers</h3>
                <p class="fragment">Normalization layers</p>
                <ul class="fragment">
                    <li>nn.BatchNorm1d</li>
                    <li>nn.BatchNorm2d</li>
                    <li>nn.BatchNorm3d</li>
                    <li>nn.GroupNorm</li>
                </ul>
            </section>

            <section>
                <h3 data-id="code-title2">Popular computer vision layers</h3>
                <p class="fragment">Non-linear Activations</p>
                <ul class="fragment">
                    <li>nn.ReLU</li>
                    <li>nn.Sigmoid</li>
                </ul>
            </section>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Convolution layer</p>
            <pre data-id="code-animation4" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
torch.nn.Conv2d(
                in_channels, 
                out_channels, 
                kernel_size, 
                stride=1, 
                padding=0, 
                dilation=1, 
                groups=1, 
                bias=True, 
                padding_mode='zeros')
                </code>
            </pre>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Dimension calculation</p>
            <div class="fragment">
                <script type="math/tex">
                - Input: \left(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}}\right)\\
                - Output: \left(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}}\right) where\\
                H_{out}=\left\lfloor\frac{H_{in}+2 \times \operatorname{padding}[0]-\operatorname{dilation}[0] \times(\operatorname{kernel\_size}[0]-1)-1}{\operatorname{stride}[0]}+1\right\rfloor\\
                W_{out}=\left\lfloor\frac{W_{in}+2 \times \operatorname{padding}[1]-\operatorname{dilation}[1] \times(\operatorname{kernel\_size}[1]-1)-1}{\operatorname{stride}[1]}+1\right\rfloor\\
                </script>
            </div>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Examples</p>
            <pre data-id="code-animation5" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
# With square kernels and equal stride
m = nn.Conv2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
input = torch.randn(20, 16, 50, 100)
output = m(input)
                </code>
            </pre>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title4">Exercise 1</h3>
            <p class="fragment" style="text-align: left;">Create a 4-dimensional tensor 'a' with (N, C, H, W) = (4,4,4,4), a convolution layer 'conv' with kernel size of 3, 
                stride sets to 1 and padding set to 0, output channel is 32, then print out the shape of values of output tensor after 'a' goes through 'conv'.
            </p>
            <pre data-id="code-animation6" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
a = torch.randn(4, 4, 4, 4)
conv = nn.Conv2d(4, 32, kernel_size=3, stride=1, padding=0)
b = conv(a)
print(b.shape)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Pooling layers</p>
            <pre data-id="code-animation7" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, 
                    return_indices=False, ceil_mode=False)
torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, 
                    count_include_pad=True, divisor_override=None)
torch.nn.AdaptiveMaxPool2d(output_size,return_indices=False)
torch.nn.AdaptiveAvgPool2d(output_size)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title3">Implementation details</h3>
            <p class="fragment">Dimension calculation (non adaptive)</p>
            <div class="fragment">
                <script type="math/tex">
                    - Input: \left(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}}\right)\\
                    - Output: \left(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}}\right) where\\
                    H_{out}=\left\lfloor\frac{H_{in}+2 \times \operatorname{padding}[0]-\operatorname{dilation}[0] \times(\operatorname{kernel\_size}[0]-1)-1}{\operatorname{stride}[0]}+1\right\rfloor\\
                    W_{out}=\left\lfloor\frac{W_{in}+2 \times \operatorname{padding}[1]-\operatorname{dilation}[1] \times(\operatorname{kernel\_size}[1]-1)-1}{\operatorname{stride}[1]}+1\right\rfloor\\
                </script>
            </div>
        </section>
        
        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title5">Exercise 2</h3>
            <p class="fragment" style="text-align: left;"> Using the output of exercise 1, create a max pooling layer 'pool' with kernel sets to 2, padding equals 0, 
                print out the shape of values of output layer after 'b' goes through 'pool'.
            </p>
            <pre data-id="code-animation8" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
pool = nn.MaxPool2d(kernel_size=2, padding=0)
c = pool(b)
print(c.shape)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title6">One more thing, I need Tensorboard logging</h3>
            <pre data-id="code-animation9" class="fragment"><code class="hljs" data-trim data-line-numbers="|1-2|5|12|16-19">
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

for epoch in range(EPOCHS):
    loss = 0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()

        outputs = net(inputs)
        # Compute loss function
        loss += criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if phase = "training":
            writer.add_scalar('Loss/train', loss)
        elif phase = "test":
            writer.add_scalar('Loss/test', loss)
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h4 data-id="code-title7">Actually, I would rather use W&B than Tensorboard, can we change that?</h4>
            <pre data-id="code-animation10" class="fragment"><code class="hljs" data-trim data-line-numbers="|1|5-8">
import wandb
for epoch in range(EPOCHS):
    ...

    if phase = "training":
        wandb.log({'epoch': epoch, 'train_loss': loss})
    elif phase = "test":
        wandb.log({'epoch': epoch, 'test_loss': loss})
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title8">You know what?</h3>

            <p data-id="code-title9">Nvidia gave us a GPU</p>

            <p data-id="code-title10">Can we run this code using CUDA?</p>

            <pre data-id="code-animation11" class="fragment"><code class="hljs" data-trim
                                                                 data-line-numbers="1, 2, 7, 8">

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device) # or model = model.cuda()

for epoch in range(EPOCHS):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        inputs = inputs.to(device) # or inputs = inputs.cuda()
        labels = labels.to(device) # or labels = labels.cuda()

        optimizer.zero_grad()

        outputs = net(inputs)
        loss += criterion(outputs, labels)  # Compute loss function
        ...
					</code></pre>
        </section>

        <section data-background-transition="zoom">
            <h3 data-id="code-title11">And how to run that on multiple GPUs?</h3>
            <pre data-id="code-animation12" class="fragment"><code class="hljs" data-trim
                                                                 data-line-numbers="1, 2, 7, 8">
model = model.to(device) # Not this one
model = nn.DataParallel(model, device_ids=[0,1,2,3]) # This
					</code></pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <section>
                <h3 data-id="code-title12">Exercise 3</h3>
                <p style="text-align: left;"> Create a model contains:
                    <ul>
                        <li>2 convolution layers with kernel size of 3</li>
                        <li>1 max pooling layer with kernel size of 3 and stride 2</li>
                        <li>1 ReLU layer</li>
                        <li>1 batch normalization layer</li>
                        <li>1 linear layer with 10 outputs</li>
                    </ul>
                    <div></div>
                    Define forward flow as 
                    <br>'conv1' -> 'bn' -> 'relu' -> 'pool' -> 'conv2' -> 'bn' -> 'relu' -> 'pool' -> 'fc'</br> 
                    Know that input size is (3,32,32)
                </p>
            </section>
            <section>
                <h3 data-id="code-title12">Exercise 3</h3>
                <pre data-id="code-animation13">
                    <code class="hljs" data-trim data-line-numbers="4|5|6|7|8|9|4-9|10-21|">
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(3, 16, 3)
            self.pool = nn.MaxPool2d(3, 2)
            self.conv2 = nn.Conv2d(16, 32, 3)
            self.fc1 = nn.Linear(32 * 5 * 5, 10)
            self.bn = nn.BatchNorm2d()
            self.relu = nn.ReLU(inplace=True)
        def forward(self, x):
            x = self.conv1(x) # (3,32,32) -> (16,30,30)
            x = self.bn(x)
            x = self.relu(x)
            x = self.pool(x) # (16,30,30) -> (16,14,14)
            x = self.conv2(x) # (16,14,14) -> (32,12,12) 
            x = self.bn(x)
            x = self.relu(x)
            x = self.pool(x) # (32,12,12) -> (32,5,5)
            x = x.view(-1, 32 * 5 * 5)
            x = self.fc(x)
            return x
                    </code>
                </pre>
            </section>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title13">Computer Vision transfer learning </h3>
            <p>
                <h4>Finetuning the convnet</h4>
                <p style="text-align: left;">Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.</p>
            </p>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title13">Computer Vision transfer learning </h3>
            <p>
                <h4>ConvNet as fixed feature extractor</h4>
                <p style="text-align: left;">Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.</p>
            </p>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title13">Computer Vision transfer learning </h3>
            <h4>Finetuning the convnet</h4>
            <pre data-id="code-animation14">
                <code class="hljs" data-trim data-line-numbers>
model_ft =  models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
# Here the size of each output sample is set to 2.
# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title13">Computer Vision transfer learning </h3>
            <h4>ConvNet as fixed feature extractor</h4>
            <pre data-id="code-animation15">
                <code class="hljs" data-trim data-line-numbers>
model_conv = torchvision.models.resnet18(pretrained=True)
for param in model_conv.parameters():
    param.requires_grad = False

# Parameters of newly constructed modules have requires_grad=True by default
num_ftrs = model_conv.fc.in_features
model_conv.fc = nn.Linear(num_ftrs, 2)

model_conv = model_conv.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that only parameters of final layer are being optimized as
# opposed to before.
optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)
                </code>
            </pre>
        </section>

        <section data-background-transition="zoom" data-auto-animate>
            <h3 data-id="code-title14">Exercise 4</h3>
            <p class="fragment" style="text-align: left;"> Create a model with pretrained Resnet-50 as backbone as features extractor, change the output dim of 'fc' to 10
            </p>
            <pre data-id="code-animation16" class="fragment">
                <code class="hljs" data-trim data-line-numbers>
model = torchvision.models.resnet50(pretrained=True)
for param in model.parameters():
    param.requires_grad = False

# Parameters of newly constructed modules have requires_grad=True by default
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 10)
                </code>
            </pre>
        </section>

        <section data-auto-animate>
            <h3 data-id="code-title15">More ...</h3>

            <p class="fragment">I heard that the lr scheduler is a cool feature</p>

            <p class="fragment">Can we add that?</p>

            <pre data-id="code-animation17" class="fragment"><code class="hljs" data-trim data-line-numbers="1, 11">
scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
for epoch in range(EPOCHS):
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()
					</code></pre>
        </section>

        <section data-auto-animate>
            <h3 data-id="code-title16">And ...</h3>

            <p class="fragment">We should use transformations for images, right?</p>

            <p class="fragment">Can we add that?</p>

            <pre data-id="code-animation18" class="fragment"><code class="hljs" data-trim data-line-numbers="1|2-14|16-23">
from torchvision import datasets, transforms
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = 'data/hymenoptera_data'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
					</code></pre>
        </section>

        <section data-background-transition="zoom">
            <h2>Some useful Github repository</h2>
            <h3>Classification models</h3>
            <ul style="text-align: left;">
                <li><a href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a></li>
                <li><a href="https://github.com/osmr/imgclsmob">https://github.com/osmr/imgclsmob</li>https://github.com/open-mmlab/mmsegmentation</a>
                <li><a href="https://github.com/open-mmlab/mmclassification">https://github.com/open-mmlab/mmclassification</a></li>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h2>Some useful Github repository</h2>
            <h3>object detection models</h3>
            <ul style="text-align: left;">
                <li><a href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</a></li>
                <li><a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h2>Some useful Github repository</h2>
            <h3>Distillation</h3>
            <ul style="text-align: left;">
                <li><a href="https://github.com/IntelLabs/distiller">https://github.com/IntelLabs/distiller</a></li>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Plotting neural networks</h3>
            <ul style="text-align: left;">
                <li>
                    ğ‘·ğ’ğ’ğ’•ğ‘µğ’†ğ’–ğ’“ğ’‚ğ’ğ‘µğ’†ğ’• : using latex to visualize neural network for paper, slide... 
                </li>
                <p>Github: <a href="https://github.com/HarisIqbal88/PlotNeuralNet">https://github.com/HarisIqbal88/PlotNeuralNet</a> </p>
                <p>Demo FCN-32 : <a href="https://www.overleaf.com/read/wsxpmkqvjnbs">https://www.overleaf.com/read/wsxpmkqvjnbs</a> </p>
                <p>Demo Holistically-Nested Edge Detection: <a href="https://www.overleaf.com/read/jxhnkcnwhfxp">https://www.overleaf.com/read/jxhnkcnwhfxp</a> </p>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Plotting neural networks</h3>
            <ul style="text-align: left;">
                <li>
                    ğ’ğ’†ğ’•2ğ’—ğ’Šğ’” : visualize neural network architecture from model Keras
                </li>
                <p>Github: <a href="https://github.com/tensorflow/tensorboard">https://github.com/viscom-ulm/Net2Vis</a> </p> 
                <p>Demo: <a href="https://github.com/tensorflow/tensorboard">https://viscom.net2vis.uni-ulm.de/</a> </p>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Plotting neural networks</h3>
            <ul style="text-align: left;">
                <li>
                    ğ’—ğ’Šğ’”ğ’–ğ’‚ğ’ğ’Œğ’†ğ’“ğ’‚ğ’” is similar to ğ’ğ’†ğ’•2ğ’—ğ’Šğ’”
                </li>
                <p>Github: <a href="https://github.com/paulgavrikov/visualkeras/">https://github.com/paulgavrikov/visualkeras/</a> </p>
                <li>
                    ğ’…ğ’“ğ’‚ğ’˜_ğ’„ğ’ğ’ğ’—ğ’ğ’†ğ’• : visualize Convolution Neural Network
                </li>
                <p>Github: <a href="https://github.com/gwding/draw_convnet">https://github.com/gwding/draw_convnet</a> </p>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Plotting neural networks</h3>
            <ul style="text-align: left;">
                <li>
                    ğ‘µğ‘µ-ğ‘ºğ‘½ğ‘® 
                </li>
                <p>Github: <a href="https://github.com/alexlenail/NN-SVG">https://github.com/alexlenail/NN-SVG</a> </p>
                <p>Demo : <a href="http://alexlenail.me/NN-SVG/AlexNet.html">http://alexlenail.me/NN-SVG/AlexNet.html</a> </p>
                <li>
                    ğ‘»ğ’†ğ’ğ’”ğ’ğ’“ğ’ƒğ’ğ’‚ğ’“ğ’… : visualize neural network from Tensorflow graph.
                </li>
                <p>Github: <a href="https://github.com/tensorflow/tensorboard">https://github.com/tensorflow/tensorboard</a></p>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Some useful website</h3>
            <ul>
                <li><a href="https://paperswithcode.com/">https://paperswithcode.com/</a></li>
                <li><a href="https://www.connectedpapers.com/</">https://www.connectedpapers.com/</a></li>
                <li><a href="https://sh-tsang.medium.com/">https://sh-tsang.medium.com/</a></li>
                <li><a href="https://towardsdatascience.com/">https://towardsdatascience.com/</a></li>
                <li><a href="https://jonathan-hui.medium.com/">https://jonathan-hui.medium.com/</a></li>
                <li><a href="https://papertalk.org/index/">https://papertalk.org/papertalks/</a></li>
            </ul>
        </section>

        <section data-background-transition="zoom">
            <h3>Thank you for your time</h3>
            <p>Feel free to ask any question</p>
            <p>
                <small>Binh Nguyen</small><br/>
                <small>Presentation available at:<br> <a
                        href="https://beandkay.github.io/#">beandkay.github.io/#</a></small><br/>
            </p>
        </section>
    </div>


    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/math/math.js"></script>
	<script src="../plugin/math/math.js"></script>
    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        hash: true,
        controls: true,
        progress: true,
        slideNumber: true,
        overview: true,
        center: true,
        navigationMode: 'default',
        fragmentInURL: false,
        embedded: false,
        preloadIframes: null,
        autoSlide: 0,
        autoSlideStoppable: true,
        defaultTiming: 120,
        mouseWheel: false,
        previewLinks: false,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'fast', // default/fast/slow
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        display: 'block',
        math: {
          mathjax: 'plugin/math/MathJax.js',
          config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
          // pass other options into `MathJax.Hub.Config()`
          TeX: {
              Macros: {
                    RR: '{\\bf R}',
                    R: '\\mathbb{R}',
                    set: [ '\\left\\{#1 \\; ; \\; #2\\right\\}', 2 ]
                }
            },
        },
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],
      });
    </script>
</body>
</html>




